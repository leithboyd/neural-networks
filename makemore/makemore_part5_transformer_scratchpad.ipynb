{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_mul(data, transform):\n",
    "    '''\n",
    "    data: batch, F, FD\n",
    "    transform: heads, FD, HFD\n",
    "    result: (batch, heads, F, HFD)\n",
    "\n",
    "    '''\n",
    "    assert (tf.rank(data) == 3)\n",
    "    assert (tf.rank(transform) == 3)\n",
    "    assert (data.shape[2] == transform.shape[1])\n",
    "\n",
    "    return tf.expand_dims(data, 1) @ transform\n",
    "\n",
    "\n",
    "def head_mul_simple(data, transform):\n",
    "    '''\n",
    "    data: batch, F, FD\n",
    "    transform: heads, FD, HFD\n",
    "    result: (batch, heads, F, HFD)\n",
    "    '''\n",
    "    assert (tf.rank(data) == 3)\n",
    "    assert (tf.rank(transform) == 3)\n",
    "    assert (data.shape[2] == transform.shape[1])\n",
    "\n",
    "    B, F, FD = data.shape\n",
    "    H, _, HFD = transform.shape\n",
    "\n",
    "    result = np.zeros((B, H, F, HFD))\n",
    "\n",
    "    for b in range(B):\n",
    "        for h in range(H):\n",
    "            for f in range(F):\n",
    "                for fd in range(HFD):\n",
    "                    result[b, h, f, fd] = tf.tensordot(data[b, f, :],  transform[h, :, fd], 1).numpy()\n",
    "\n",
    "    return tf.convert_to_tensor(result, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, F, FD = 8, 5, 4\n",
    "heads, HFD = 3, 2\n",
    "\n",
    "data = tf.random.normal((batch, F, FD))\n",
    "transform = tf.random.normal((heads,  FD, HFD))\n",
    "\n",
    "result =  head_mul(data, transform)\n",
    "\n",
    "tf.debugging.assert_near(result, head_mul_simple(data, transform))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 3, 5, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, F, FD = 8, 5, 4\n",
    "heads, HFD = 3, 2\n",
    "\n",
    "data = tf.random.normal((batch, F, FD))\n",
    "key_transform = tf.random.normal((heads,  FD, HFD))\n",
    "value_transform = tf.random.normal((heads,  FD, HFD))\n",
    "query_transform = tf.random.normal((heads,  FD, HFD))\n",
    "\n",
    "key = head_mul(data, key_transform)\n",
    "value = head_mul(data, value_transform)\n",
    "query = head_mul(data, query_transform)\n",
    "\n",
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[ 5.8921366 , -8.357531  ,  6.0643506 ,  5.5240784 ,  7.334341  ],\n",
       "       [-5.014019  ,  8.240324  , -4.897439  , -4.9543586 , -7.053102  ],\n",
       "       [ 2.7879987 , -2.0333219 ,  3.3175197 ,  2.1821284 ,  2.0881238 ],\n",
       "       [-0.5884373 ,  2.2399423 , -0.27792072, -0.86745894, -1.7435443 ],\n",
       "       [ 1.438715  , -1.8663764 ,  1.5214187 ,  1.3096718 ,  1.6654413 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(query, key):\n",
    "    '''\n",
    "    query: (batch, heads, F, FD)\n",
    "    key: (batch, heads, F, FD)\n",
    "    result: (batch, heads, F (query), F (key))\n",
    "    '''\n",
    "    assert (tf.rank(query) == 4)\n",
    "    assert (query.shape == key.shape)\n",
    "\n",
    "    return query @ tf.transpose(key, (0, 1, 3, 2))\n",
    "\n",
    "\n",
    "def attention_simple(query, key):\n",
    "    '''\n",
    "    query: (batch, heads, F, FD)\n",
    "    key: (batch, heads, F, FD)\n",
    "    result: (batch, heads, F (query), F (key))\n",
    "    '''\n",
    "    assert (tf.rank(query) == 4)\n",
    "    assert (query.shape == key.shape)\n",
    "\n",
    "    B, H, F, _ = query.shape\n",
    "    result = np.zeros((B, H, F, F))\n",
    "\n",
    "    for b in range(B):\n",
    "        for h in range(H):\n",
    "            for q in range(F):\n",
    "                for k in range(F):\n",
    "                    result[b, h, q, k] =  tf.tensordot(query[b, h, q, :], key[b, h, k, :], 1).numpy()\n",
    "    \n",
    "    return tf.convert_to_tensor(result, dtype=tf.float32)\n",
    "\n",
    "attention_simple(query, key)[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention\n",
      "[[ 5.8921366  -8.357531    6.0643506   5.5240784   7.334341  ]\n",
      " [-5.014019    8.240324   -4.897439   -4.9543586  -7.053102  ]\n",
      " [ 2.7879987  -2.0333219   3.3175197   2.1821284   2.0881238 ]\n",
      " [-0.5884373   2.2399423  -0.27792072 -0.86745894 -1.7435443 ]\n",
      " [ 1.438715   -1.8663764   1.5214187   1.3096718   1.6654413 ]]\n",
      "\n",
      "lower diagional mask\n",
      "[[ True False False False False]\n",
      " [ True  True False False False]\n",
      " [ True  True  True False False]\n",
      " [ True  True  True  True False]\n",
      " [ True  True  True  True  True]]\n",
      "\n",
      "masked attention\n",
      "[[ 5.8921366         -inf        -inf        -inf        -inf]\n",
      " [-5.014019    8.240324          -inf        -inf        -inf]\n",
      " [ 2.7879987  -2.0333219   3.3175197         -inf        -inf]\n",
      " [-0.5884373   2.2399423  -0.27792072 -0.86745894        -inf]\n",
      " [ 1.438715   -1.8663764   1.5214187   1.3096718   1.6654413 ]]\n",
      "\n",
      "attention normalised with softmax\n",
      "[[1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.7527145e-06 9.9999821e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [3.6952525e-01 2.9769482e-03 6.2749779e-01 0.0000000e+00 0.0000000e+00]\n",
      " [4.9903493e-02 8.4426850e-01 6.8074830e-02 3.7753161e-02 0.0000000e+00]\n",
      " [2.3494373e-01 8.6214626e-03 2.5520056e-01 2.0650052e-01 2.9473373e-01]]\n",
      "\n",
      "sum accross rows (should be 1)\n",
      "[1.         0.99999994 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "att = attention(query, key)\n",
    "\n",
    "tf.debugging.assert_near(att, attention_simple(query, key))\n",
    "\n",
    "print('attention')\n",
    "print(att[0, 0, :, :].numpy())\n",
    "\n",
    "lower_diag_mask = tf.linalg.band_part(tf.ones((F, F), dtype=tf.bool), -1, 0)\n",
    "\n",
    "print('\\nlower diagional mask')\n",
    "print(lower_diag_mask.numpy())\n",
    "\n",
    "att = tf.where(lower_diag_mask, att, float('-inf'))\n",
    "\n",
    "print('\\nmasked attention')\n",
    "print(att[0, 0, :, :].numpy())\n",
    "\n",
    "att = tf.keras.activations.softmax(att, axis = 3)\n",
    "\n",
    "print('\\nattention normalised with softmax')\n",
    "print(att[0, 0, :, :].numpy())\n",
    "\n",
    "\n",
    "print('\\nsum accross rows (should be 1)')\n",
    "print(tf.reduce_sum(att[0, 0, :, :], 1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att\n",
      "[[1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.7527145e-06 9.9999821e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [3.6952525e-01 2.9769482e-03 6.2749779e-01 0.0000000e+00 0.0000000e+00]\n",
      " [4.9903493e-02 8.4426850e-01 6.8074830e-02 3.7753161e-02 0.0000000e+00]\n",
      " [2.3494373e-01 8.6214626e-03 2.5520056e-01 2.0650052e-01 2.9473373e-01]]\n",
      "\n",
      "value\n",
      "[[-0.15064985  0.65298265]\n",
      " [-1.7147651  -3.2876022 ]\n",
      " [-0.8068951   1.0620738 ]\n",
      " [-0.07752123  2.2077055 ]\n",
      " [ 0.94131666  3.1736104 ]]\n",
      "\n",
      "res = att @ value\n",
      "[[-0.15064985  0.65298265]\n",
      " [-1.7147622  -3.2875953 ]\n",
      " [-0.56709856  0.89795554]\n",
      " [-1.5130961  -2.5873847 ]\n",
      " [ 0.00533149  1.7873744 ]]\n",
      "\n",
      "feature 0 dims for all heads\n",
      "[[-0.15064985  0.65298265]\n",
      " [-0.16928649 -0.9094292 ]\n",
      " [ 1.8130186   2.3197675 ]]\n",
      "\n",
      "feature 0 dims for all heads stacked\n",
      "[-0.15064985  0.65298265 -0.16928649 -0.9094292   1.8130186   2.3197675 ]\n"
     ]
    }
   ],
   "source": [
    "print('att')\n",
    "print(att[0, 0, :, :].numpy())\n",
    "\n",
    "print('\\nvalue')\n",
    "print(value[0, 0, :, :].numpy())\n",
    "\n",
    "res = att @ value\n",
    "\n",
    "print('\\nres = att @ value')\n",
    "print(res[0, 0, :, :].numpy())\n",
    "\n",
    "print('\\nfeature 0 dims for all heads')\n",
    "print(res[0, :, 0, :].numpy())\n",
    "\n",
    "res = tf.transpose(res, (0, 2, 1, 3))\n",
    "B, F, H, FD =  res.shape\n",
    "res = tf.reshape(res, (B, F, H*FD))\n",
    "\n",
    "print('\\nfeature 0 dims for all heads stacked')\n",
    "print(res[0, 0, :].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
